{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1c_retropropagacion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gibranfp/CursoAprendizajeProfundo/blob/master/notebooks/1c_retropropagacion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V83__FrBij1f"
      },
      "source": [
        "# Retropropagación\n",
        "\n",
        "En este *notebook* programaremos con NumPy una red neuronal densa y la entrenaremos para aproximar la operación XOR usando del gradiente descedente con el algoritmo de retropropagación. Recordemos que la operación XOR ($\\otimes$) está de la siguiente manera:\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$\n",
        "| ------------- |:-------------:| -----:|\n",
        "|0 |0 |0|\n",
        "|0 |1 |1|\n",
        "|1 |0 |1|\n",
        "|1 |1 |0|\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSlnjW4Oi-FP"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iAUmKI5jNuX"
      },
      "source": [
        "Nuestra red neuronal densa está compuesta por una capa de 2 entradas ($x_1$ y $x_2$), una capa oculta con 10 neuronas con función de activación sigmoide y una capa de salida con una sola neurona con función de activación sigmoide. Esta función de activación se define como:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYhT3i68jf6x"
      },
      "source": [
        "def sigmoide(z):\n",
        "    return 1 / (1 + np.exp(-z))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx6SyrPhWBrw"
      },
      "source": [
        "La función sigmoide tiene una derivada que está expresada en términos de la misma función, esto es, \n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\sigma (z)}{\\partial z} = \\sigma(z) (1 - \\sigma(z))\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJxvxKeAjn24"
      },
      "source": [
        "def derivada_sigmoide(x):\n",
        "    return np.multiply(sigmoide(x), (1.0 - sigmoide(x)))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WxI8FfLXKHv"
      },
      "source": [
        "Podemos ver la operación XOR como una tarea de clasificación binaria a partir de 2 entradas. Por lo tanto, usaremos la función de pérdida de entropía cruzada binaria:\n",
        "\n",
        "$$\n",
        "ECB(\\mathbf{y}, \\mathbf{\\hat{y}})  = -\\sum_{i=1}^N \\left[ y^{(i)} \\log \\hat{y}^{(i)} + (1 - y^{(i)}) \\log (1 - \\hat{y}^{(i)}) \\right]\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDjlmpAQjR3X"
      },
      "source": [
        "def entropia_cruzada_binaria(y, p):\n",
        "    p[p == 0] = np.nextafter(0., 1.)\n",
        "    p[p == 1] = np.nextafter(1., 0.)\n",
        "    return -(np.log(p[y == 1]).sum() + np.log(1 - p[y == 0]).sum())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8nMdK-RYWMS"
      },
      "source": [
        "Asimismo, calcularemos la exactitud para medir el rendimiento del modelo aprendido por la red neuronal densa:\n",
        "\n",
        "$$\n",
        "exactitud = \\frac{correctos}{total}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wxvZq10jIM3"
      },
      "source": [
        "def exactitud(y, y_predicha):\n",
        "    return (y == y_predicha).mean() * 100"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p02hAdUFZNLL"
      },
      "source": [
        "Ahora, definimos la función que propaga hacia adelante una entrada $\\mathbf{x}^{i}$. Como la red está compuesta de 2 capas densas (1 oculta y 1 de salida), tenemos 2 matrices de pesos con sus correspondientes vectores de sesgos $\\{\\mathbf{W}^{\\{1\\}}, \\mathbf{b}^{\\{1\\}}\\}$ y $\\{\\mathbf{W}^{\\{2\\}}, \\mathbf{b}^{\\{2\\}}\\}$ de la capa oculta y la capa de salida respectivamente. Así, podemos llevar a cabo la propagación hacia adelante en esta red de la siguiente manera:\n",
        "\n",
        "$$\n",
        "\t\\begin{split}\n",
        "\t\t\t\t\\mathbf{a}^{\\{1\\}} & =  \\mathbf{x}^{(i)} \\\\\n",
        "\t\t\t\t\\mathbf{z}^{\\{2\\}} & =  \\mathbf{W}^{\\{1\\}} \\cdot \\mathbf{a}^{\\{1\\}} + \\mathbf{b}^{\\{1\\}}\\\\\n",
        "\t\t\t\t\\mathbf{a}^{\\{2\\}} & =  \\sigma(\\mathbf{z}^{\\{2\\}}) \\\\\n",
        "\t\t\t\t\\mathbf{z}^{\\{3\\}} & =  \\mathbf{W}^{\\{2\\}} \\cdot \\mathbf{a}^{\\{2\\}}  + \\mathbf{b}^{\\{2\\}}\\\\\n",
        "\t\t\t\t\\mathbf{a}^{\\{3\\}} & =  \\sigma(\\mathbf{z}^{\\{3\\}})\\\\\n",
        "\t\t\t\t\\hat{y}^{(i)} & =  \\mathbf{a}^{\\{3\\}}\n",
        "\t\t\t\\end{split}\n",
        "      $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAsEk-zajvpX"
      },
      "source": [
        "def hacia_adelante(x, W1, b1, W2, b2):\n",
        "  z2 = np.dot(W1.T, x[:, np.newaxis]) + b1\n",
        "  a2 = sigmoide(z2)\n",
        "  z3 = np.dot(W2.T, a2) + b2\n",
        "  y_hat = sigmoide(z3)\n",
        "  \n",
        "  return z2, a2, z3, y_hat"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiOT6jqXjzwQ"
      },
      "source": [
        "Finalmente, definimos la función para entrenar nuestra red neuronal usando gradiente descendente. Para calcular el gradiente de la función de pérdida respecto a los pesos y sesgos en cada capa empleamos el algoritmo de retropropagación.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P7i6eLgkJdg"
      },
      "source": [
        "def retropropagacion(X, y, alpha = 0.01, n_epocas = 100, n_ocultas = 10):\n",
        "    n_ejemplos = X.shape[0]\n",
        "    n_entradas = X.shape[1]\n",
        "    \n",
        "    # Inicialización de las matrices de pesos W y V\n",
        "    W1 = np.sqrt(1.0 / n_entradas) * np.random.randn(n_entradas, n_ocultas)\n",
        "    b1 = np.zeros((n_ocultas, 1))\n",
        "    \n",
        "    W2 = np.sqrt(1.0 / n_ocultas) * np.random.randn(n_ocultas, 1)\n",
        "    b2 = np.zeros((1, 1))\n",
        "    \n",
        "    perdidas = np.zeros((n_epocas))\n",
        "    exactitudes = np.zeros((n_epocas))\n",
        "    y_predicha = np.zeros((y.shape))\n",
        "    for i in range(n_epocas):\n",
        "        for j in range(n_ejemplos):\n",
        "            z2, a2, z3, y_hat = hacia_adelante(X[j], W1, b1, W2, b2)\n",
        "\n",
        "            # cálculo de gradiente para W2 por retropropagación\n",
        "            delta3 = (y_hat - y[j]) * derivada_sigmoide(z3)\n",
        "            W2 = W2 - alpha * np.outer(a2, delta3)\n",
        "            b2 = b2 - alpha * delta3\n",
        "\n",
        "            # cálculo de gradiente para W1 por retropropagación\n",
        "            delta2 = np.dot(W2, delta3) * derivada_sigmoide(z2)\n",
        "            W1 = W1 - alpha * np.outer(X[j], delta2)\n",
        "            b1 = b1 - alpha * delta2\n",
        "\n",
        "            y_predicha[j] = y_hat\n",
        "            \n",
        "        # calcula la pérdida en la época\n",
        "        perdidas[i] = entropia_cruzada_binaria(y, y_predicha)\n",
        "        exactitudes[i] = exactitud(y, np.round(y_predicha))\n",
        "        print('Epoch {0}: Pérdida = {1} Exactitud = {2}'.format(i, \n",
        "                                                              perdidas[i], \n",
        "                                                              exactitudes[i]))\n",
        "\n",
        "    return W1, W2, perdidas, exactitudes"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nau0HWsrkRxg"
      },
      "source": [
        "Para probar nuestra red, generamos los ejemplos correspondientes a la operación XOR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8txXZ34GkUAF"
      },
      "source": [
        "# ejemplo (XOR)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0, 1, 1, 0]]).T"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLT8avfhkYH7"
      },
      "source": [
        "Finalmente, entrenamos nuestra red con estos ejemplos por 200 épocas usando una tasa de aprendizaje $\\alpha = 1.0$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijKxVwZ3kbyR",
        "outputId": "85e096e7-dfc5-45c0-9e71-b7512775c50f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "np.random.seed(0)\n",
        "W1, W2, perdidas, exactitudes = retropropagacion(X, \n",
        "                                                 y, \n",
        "                                                 alpha = 1.0, \n",
        "                                                 n_epocas = 200,\n",
        "                                                 n_ocultas = 10)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: Pérdida = 3.4737521571212318 Exactitud = 25.0\n",
            "Epoch 1: Pérdida = 3.4621893539983217 Exactitud = 25.0\n",
            "Epoch 2: Pérdida = 3.4638591836303827 Exactitud = 50.0\n",
            "Epoch 3: Pérdida = 3.4667990850901718 Exactitud = 50.0\n",
            "Epoch 4: Pérdida = 3.4692765311295695 Exactitud = 50.0\n",
            "Epoch 5: Pérdida = 3.4712999939289766 Exactitud = 50.0\n",
            "Epoch 6: Pérdida = 3.4730576791773418 Exactitud = 50.0\n",
            "Epoch 7: Pérdida = 3.474680551473296 Exactitud = 50.0\n",
            "Epoch 8: Pérdida = 3.4762395683680385 Exactitud = 50.0\n",
            "Epoch 9: Pérdida = 3.477769475088765 Exactitud = 50.0\n",
            "Epoch 10: Pérdida = 3.4792860090889137 Exactitud = 50.0\n",
            "Epoch 11: Pérdida = 3.4807954729134245 Exactitud = 50.0\n",
            "Epoch 12: Pérdida = 3.4822996094619056 Exactitud = 50.0\n",
            "Epoch 13: Pérdida = 3.483797989727295 Exactitud = 50.0\n",
            "Epoch 14: Pérdida = 3.485289159117471 Exactitud = 50.0\n",
            "Epoch 15: Pérdida = 3.48677118131892 Exactitud = 50.0\n",
            "Epoch 16: Pérdida = 3.4882418941046383 Exactitud = 50.0\n",
            "Epoch 17: Pérdida = 3.4896990286323075 Exactitud = 50.0\n",
            "Epoch 18: Pérdida = 3.4911402644339606 Exactitud = 50.0\n",
            "Epoch 19: Pérdida = 3.4925632542361824 Exactitud = 50.0\n",
            "Epoch 20: Pérdida = 3.493965634659588 Exactitud = 50.0\n",
            "Epoch 21: Pérdida = 3.4953450303033025 Exactitud = 50.0\n",
            "Epoch 22: Pérdida = 3.496699054706433 Exactitud = 50.0\n",
            "Epoch 23: Pérdida = 3.4980253098019345 Exactitud = 50.0\n",
            "Epoch 24: Pérdida = 3.4993213846049835 Exactitud = 50.0\n",
            "Epoch 25: Pérdida = 3.500584853474096 Exactitud = 25.0\n",
            "Epoch 26: Pérdida = 3.5018132740977728 Exactitud = 25.0\n",
            "Epoch 27: Pérdida = 3.5030041852753735 Exactitud = 25.0\n",
            "Epoch 28: Pérdida = 3.5041551045237114 Exactitud = 25.0\n",
            "Epoch 29: Pérdida = 3.5052635255251907 Exactitud = 25.0\n",
            "Epoch 30: Pérdida = 3.506326915427653 Exactitud = 25.0\n",
            "Epoch 31: Pérdida = 3.5073427120050202 Exactitud = 25.0\n",
            "Epoch 32: Pérdida = 3.5083083206889745 Exactitud = 25.0\n",
            "Epoch 33: Pérdida = 3.5092211114841056 Exactitud = 25.0\n",
            "Epoch 34: Pérdida = 3.510078415781761 Exactitud = 25.0\n",
            "Epoch 35: Pérdida = 3.5108775230909806 Exactitud = 25.0\n",
            "Epoch 36: Pérdida = 3.511615677708261 Exactitud = 25.0\n",
            "Epoch 37: Pérdida = 3.512290075351479 Exactitud = 25.0\n",
            "Epoch 38: Pérdida = 3.512897859786942 Exactitud = 25.0\n",
            "Epoch 39: Pérdida = 3.5134361194823702 Exactitud = 25.0\n",
            "Epoch 40: Pérdida = 3.51390188432237 Exactitud = 25.0\n",
            "Epoch 41: Pérdida = 3.514292122426847 Exactitud = 25.0\n",
            "Epoch 42: Pérdida = 3.5146037371165555 Exactitud = 25.0\n",
            "Epoch 43: Pérdida = 3.514833564073667 Exactitud = 25.0\n",
            "Epoch 44: Pérdida = 3.514978368748766 Exactitud = 25.0\n",
            "Epoch 45: Pérdida = 3.5150348440689525 Exactitud = 25.0\n",
            "Epoch 46: Pérdida = 3.5149996085047386 Exactitud = 25.0\n",
            "Epoch 47: Pérdida = 3.514869204556015 Exactitud = 25.0\n",
            "Epoch 48: Pérdida = 3.5146400977195467 Exactitud = 25.0\n",
            "Epoch 49: Pérdida = 3.514308676002087 Exactitud = 25.0\n",
            "Epoch 50: Pérdida = 3.513871250044213 Exactitud = 25.0\n",
            "Epoch 51: Pérdida = 3.5133240539203663 Exactitud = 25.0\n",
            "Epoch 52: Pérdida = 3.5126632466801118 Exactitud = 25.0\n",
            "Epoch 53: Pérdida = 3.5118849146944426 Exactitud = 25.0\n",
            "Epoch 54: Pérdida = 3.5109850748687297 Exactitud = 25.0\n",
            "Epoch 55: Pérdida = 3.5099596787808798 Exactitud = 25.0\n",
            "Epoch 56: Pérdida = 3.5088046177990435 Exactitud = 25.0\n",
            "Epoch 57: Pérdida = 3.507515729228073 Exactitud = 25.0\n",
            "Epoch 58: Pérdida = 3.506088803527576 Exactitud = 25.0\n",
            "Epoch 59: Pérdida = 3.5045195926369934 Exactitud = 25.0\n",
            "Epoch 60: Pérdida = 3.5028038194345226 Exactitud = 25.0\n",
            "Epoch 61: Pérdida = 3.5009371883470015 Exactitud = 25.0\n",
            "Epoch 62: Pérdida = 3.498915397116976 Exactitud = 25.0\n",
            "Epoch 63: Pérdida = 3.496734149721267 Exactitud = 25.0\n",
            "Epoch 64: Pérdida = 3.4943891704222536 Exactitud = 25.0\n",
            "Epoch 65: Pérdida = 3.4918762189191748 Exactitud = 25.0\n",
            "Epoch 66: Pérdida = 3.489191106551761 Exactitud = 25.0\n",
            "Epoch 67: Pérdida = 3.4863297134927738 Exactitud = 25.0\n",
            "Epoch 68: Pérdida = 3.48328800684963 Exactitud = 50.0\n",
            "Epoch 69: Pérdida = 3.4800620595782346 Exactitud = 50.0\n",
            "Epoch 70: Pérdida = 3.476648070094744 Exactitud = 50.0\n",
            "Epoch 71: Pérdida = 3.4730423824533814 Exactitud = 50.0\n",
            "Epoch 72: Pérdida = 3.4692415069406954 Exactitud = 50.0\n",
            "Epoch 73: Pérdida = 3.46524214091929 Exactitud = 50.0\n",
            "Epoch 74: Pérdida = 3.4610411897369806 Exactitud = 50.0\n",
            "Epoch 75: Pérdida = 3.4566357875012113 Exactitud = 50.0\n",
            "Epoch 76: Pérdida = 3.452023317503306 Exactitud = 50.0\n",
            "Epoch 77: Pérdida = 3.4472014320634745 Exactitud = 50.0\n",
            "Epoch 78: Pérdida = 3.4421680715554723 Exactitud = 50.0\n",
            "Epoch 79: Pérdida = 3.43692148236012 Exactitud = 50.0\n",
            "Epoch 80: Pérdida = 3.4314602334896205 Exactitud = 50.0\n",
            "Epoch 81: Pérdida = 3.425783231620567 Exactitud = 50.0\n",
            "Epoch 82: Pérdida = 3.41988973427279 Exactitud = 50.0\n",
            "Epoch 83: Pérdida = 3.4137793608743996 Exactitud = 50.0\n",
            "Epoch 84: Pérdida = 3.4074521014608403 Exactitud = 50.0\n",
            "Epoch 85: Pérdida = 3.4009083227678616 Exactitud = 50.0\n",
            "Epoch 86: Pérdida = 3.394148771495265 Exactitud = 50.0\n",
            "Epoch 87: Pérdida = 3.387174574540338 Exactitud = 50.0\n",
            "Epoch 88: Pérdida = 3.3799872360271417 Exactitud = 50.0\n",
            "Epoch 89: Pérdida = 3.37258863099012 Exactitud = 50.0\n",
            "Epoch 90: Pérdida = 3.3649809956077203 Exactitud = 50.0\n",
            "Epoch 91: Pérdida = 3.357166913923498 Exactitud = 50.0\n",
            "Epoch 92: Pérdida = 3.3491493010378535 Exactitud = 50.0\n",
            "Epoch 93: Pérdida = 3.3409313828024674 Exactitud = 50.0\n",
            "Epoch 94: Pérdida = 3.3325166721007093 Exactitud = 50.0\n",
            "Epoch 95: Pérdida = 3.323908941849719 Exactitud = 50.0\n",
            "Epoch 96: Pérdida = 3.3151121949122553 Exactitud = 50.0\n",
            "Epoch 97: Pérdida = 3.3061306311575 Exactitud = 50.0\n",
            "Epoch 98: Pérdida = 3.296968611958425 Exactitud = 50.0\n",
            "Epoch 99: Pérdida = 3.2876306224577094 Exactitud = 50.0\n",
            "Epoch 100: Pérdida = 3.278121231973205 Exactitud = 50.0\n",
            "Epoch 101: Pérdida = 3.26844505294651 Exactitud = 50.0\n",
            "Epoch 102: Pérdida = 3.258606698863189 Exactitud = 50.0\n",
            "Epoch 103: Pérdida = 3.248610741589891 Exactitud = 50.0\n",
            "Epoch 104: Pérdida = 3.2384616685815732 Exactitud = 50.0\n",
            "Epoch 105: Pérdida = 3.228163840410896 Exactitud = 50.0\n",
            "Epoch 106: Pérdida = 3.2177214490618766 Exactitud = 50.0\n",
            "Epoch 107: Pérdida = 3.20713847741141 Exactitud = 50.0\n",
            "Epoch 108: Pérdida = 3.1964186602958717 Exactitud = 50.0\n",
            "Epoch 109: Pérdida = 3.1855654475269004 Exactitud = 50.0\n",
            "Epoch 110: Pérdida = 3.1745819691814194 Exactitud = 50.0\n",
            "Epoch 111: Pérdida = 3.1634710034476163 Exactitud = 50.0\n",
            "Epoch 112: Pérdida = 3.1522349472620546 Exactitud = 50.0\n",
            "Epoch 113: Pérdida = 3.1408757899248454 Exactitud = 50.0\n",
            "Epoch 114: Pérdida = 3.129395089831119 Exactitud = 50.0\n",
            "Epoch 115: Pérdida = 3.117793954409124 Exactitud = 50.0\n",
            "Epoch 116: Pérdida = 3.1060730233091425 Exactitud = 50.0\n",
            "Epoch 117: Pérdida = 3.0942324548438425 Exactitud = 50.0\n",
            "Epoch 118: Pérdida = 3.082271915640349 Exactitud = 50.0\n",
            "Epoch 119: Pérdida = 3.070190573427409 Exactitud = 50.0\n",
            "Epoch 120: Pérdida = 3.057987092847713 Exactitud = 50.0\n",
            "Epoch 121: Pérdida = 3.045659634155522 Exactitud = 50.0\n",
            "Epoch 122: Pérdida = 3.0332058546328495 Exactitud = 50.0\n",
            "Epoch 123: Pérdida = 3.0206229125330237 Exactitud = 50.0\n",
            "Epoch 124: Pérdida = 3.007907473337795 Exactitud = 50.0\n",
            "Epoch 125: Pérdida = 2.995055718092592 Exactitud = 50.0\n",
            "Epoch 126: Pérdida = 2.982063353563217 Exactitud = 50.0\n",
            "Epoch 127: Pérdida = 2.9689256239356308 Exactitud = 50.0\n",
            "Epoch 128: Pérdida = 2.955637323757989 Exactitud = 50.0\n",
            "Epoch 129: Pérdida = 2.942192811800465 Exactitud = 50.0\n",
            "Epoch 130: Pérdida = 2.9285860254837908 Exactitud = 50.0\n",
            "Epoch 131: Pérdida = 2.9148104955024134 Exactitud = 50.0\n",
            "Epoch 132: Pérdida = 2.9008593602436803 Exactitud = 50.0\n",
            "Epoch 133: Pérdida = 2.886725379582435 Exactitud = 50.0\n",
            "Epoch 134: Pérdida = 2.8724009476130137 Exactitud = 50.0\n",
            "Epoch 135: Pérdida = 2.8578781038711103 Exactitud = 50.0\n",
            "Epoch 136: Pérdida = 2.8431485426001357 Exactitud = 50.0\n",
            "Epoch 137: Pérdida = 2.8282036196349445 Exactitud = 50.0\n",
            "Epoch 138: Pérdida = 2.8130343565151446 Exactitud = 50.0\n",
            "Epoch 139: Pérdida = 2.7976314415059496 Exactitud = 50.0\n",
            "Epoch 140: Pérdida = 2.7819852273018353 Exactitud = 50.0\n",
            "Epoch 141: Pérdida = 2.766085725321928 Exactitud = 50.0\n",
            "Epoch 142: Pérdida = 2.749922596680225 Exactitud = 50.0\n",
            "Epoch 143: Pérdida = 2.7334851401305618 Exactitud = 50.0\n",
            "Epoch 144: Pérdida = 2.7167622775465095 Exactitud = 50.0\n",
            "Epoch 145: Pérdida = 2.6997425377978783 Exactitud = 50.0\n",
            "Epoch 146: Pérdida = 2.682414040223185 Exactitud = 50.0\n",
            "Epoch 147: Pérdida = 2.6647644792630127 Exactitud = 50.0\n",
            "Epoch 148: Pérdida = 2.646781112200745 Exactitud = 50.0\n",
            "Epoch 149: Pérdida = 2.628450752339504 Exactitud = 50.0\n",
            "Epoch 150: Pérdida = 2.6097597703092896 Exactitud = 50.0\n",
            "Epoch 151: Pérdida = 2.590694106525615 Exactitud = 50.0\n",
            "Epoch 152: Pérdida = 2.5712392980886865 Exactitud = 50.0\n",
            "Epoch 153: Pérdida = 2.5513805235977562 Exactitud = 50.0\n",
            "Epoch 154: Pérdida = 2.531102669437143 Exactitud = 50.0\n",
            "Epoch 155: Pérdida = 2.5103904210480614 Exactitud = 50.0\n",
            "Epoch 156: Pérdida = 2.4892283825164983 Exactitud = 50.0\n",
            "Epoch 157: Pérdida = 2.4676012274670835 Exactitud = 50.0\n",
            "Epoch 158: Pérdida = 2.4454938837456757 Exactitud = 50.0\n",
            "Epoch 159: Pérdida = 2.4228917536919314 Exactitud = 50.0\n",
            "Epoch 160: Pérdida = 2.3997809709446702 Exactitud = 50.0\n",
            "Epoch 161: Pérdida = 2.3761486936879432 Exactitud = 50.0\n",
            "Epoch 162: Pérdida = 2.351983433039436 Exactitud = 50.0\n",
            "Epoch 163: Pérdida = 2.3272754139138696 Exactitud = 50.0\n",
            "Epoch 164: Pérdida = 2.3020169641763673 Exactitud = 50.0\n",
            "Epoch 165: Pérdida = 2.2762029262539674 Exactitud = 50.0\n",
            "Epoch 166: Pérdida = 2.2498310836250655 Exactitud = 50.0\n",
            "Epoch 167: Pérdida = 2.2229025927948394 Exactitud = 50.0\n",
            "Epoch 168: Pérdida = 2.1954224095422976 Exactitud = 50.0\n",
            "Epoch 169: Pérdida = 2.167399696463074 Exactitud = 50.0\n",
            "Epoch 170: Pérdida = 2.1388481972272437 Exactitud = 50.0\n",
            "Epoch 171: Pérdida = 2.1097865616471063 Exactitud = 50.0\n",
            "Epoch 172: Pérdida = 2.080238604759836 Exactitud = 50.0\n",
            "Epoch 173: Pérdida = 2.050233482855764 Exactitud = 75.0\n",
            "Epoch 174: Pérdida = 2.019805769923037 Exactitud = 75.0\n",
            "Epoch 175: Pérdida = 1.988995419530171 Exactitud = 75.0\n",
            "Epoch 176: Pérdida = 1.957847599893741 Exactitud = 75.0\n",
            "Epoch 177: Pérdida = 1.9264123938718298 Exactitud = 100.0\n",
            "Epoch 178: Pérdida = 1.8947443608627044 Exactitud = 100.0\n",
            "Epoch 179: Pérdida = 1.8629019638960909 Exactitud = 100.0\n",
            "Epoch 180: Pérdida = 1.830946872227379 Exactitud = 100.0\n",
            "Epoch 181: Pérdida = 1.7989431569565688 Exactitud = 100.0\n",
            "Epoch 182: Pérdida = 1.7669564039360564 Exactitud = 100.0\n",
            "Epoch 183: Pérdida = 1.7350527737955215 Exactitud = 100.0\n",
            "Epoch 184: Pérdida = 1.7032980426456608 Exactitud = 100.0\n",
            "Epoch 185: Pérdida = 1.6717566584452732 Exactitud = 100.0\n",
            "Epoch 186: Pérdida = 1.640490846916244 Exactitud = 100.0\n",
            "Epoch 187: Pérdida = 1.6095597973749924 Exactitud = 100.0\n",
            "Epoch 188: Pérdida = 1.5790189533301344 Exactitud = 100.0\n",
            "Epoch 189: Pérdida = 1.548919425820341 Exactitud = 100.0\n",
            "Epoch 190: Pérdida = 1.5193075399953386 Exactitud = 100.0\n",
            "Epoch 191: Pérdida = 1.4902245181227616 Exactitud = 100.0\n",
            "Epoch 192: Pérdida = 1.4617062956526774 Exactitud = 100.0\n",
            "Epoch 193: Pérdida = 1.4337834616103091 Exactitud = 100.0\n",
            "Epoch 194: Pérdida = 1.406481310618243 Exactitud = 100.0\n",
            "Epoch 195: Pérdida = 1.3798199912811668 Exactitud = 100.0\n",
            "Epoch 196: Pérdida = 1.353814734366729 Exactitud = 100.0\n",
            "Epoch 197: Pérdida = 1.3284761439722488 Exactitud = 100.0\n",
            "Epoch 198: Pérdida = 1.3038105354379717 Exactitud = 100.0\n",
            "Epoch 199: Pérdida = 1.2798203049229713 Exactitud = 100.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8A3KZ5JkDJ3"
      },
      "source": [
        "Graficamos el valor de la pérdida y la exactitud en cada época para ver el comportamiento de nuestra red durante el entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yglJSF9nkR7k",
        "outputId": "3339598f-a87a-469b-8499-bc16add9a559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(np.arange(perdidas.size), perdidas, label='ECB')\n",
        "plt.plot(np.arange(exactitudes.size), exactitudes, label='Exactitud')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz0OaZtsCCgs"
      },
      "source": [
        "## Inicializando los pesos con zeros\n",
        "Como se mencionó anteriormente, las matrices de pesos $\\mathbf{W^{\\{1\\}}}$ y $\\mathbf{W^{\\{2\\}}}$ se initializan con valores aleatorios pequeños mientras que los vectores de sesgo $\\mathbf{b^{\\{1\\}}}$ y $\\mathbf{b^{\\{1\\}}}$ con zeros. Examinemos qué pasa si inicializamos las matrices de pesos con zeros. Observa los valores de los pesos en cada época."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHaavbWGzqzg"
      },
      "source": [
        "def retropropagacion_zeros(X, y, alpha = 0.1, n_epocas = 100, n_ocultas = 10):\n",
        "    n_ejemplos = X.shape[0]\n",
        "    n_entradas = X.shape[1]\n",
        "    \n",
        "    # Inicializa matrices de pesos W1 y W2 y vectores de sesgos b1 y b2\n",
        "    W1 = np.zeros((n_entradas, n_ocultas))\n",
        "    b1 = np.zeros((n_ocultas, 1)) \n",
        "    W2 = np.zeros((n_ocultas, 1))\n",
        "    b2 = np.zeros((1, 1))\n",
        "    \n",
        "    perdidas = np.zeros((n_epocas))\n",
        "    exactitudes = np.zeros((n_epocas))\n",
        "    y_predicha = np.zeros((y.shape))\n",
        "    for i in range(n_epocas):\n",
        "        for j in range(n_ejemplos):\n",
        "            z2, a2, z3, y_hat = hacia_adelante(X[j], W1, b1, W2, b2)\n",
        "\n",
        "            # cálculo de gradiente para W2 por retropropagación\n",
        "            delta3 = (y[j] - y_hat) * derivada_sigmoide(z3)\n",
        "            W2 = W2 - alpha * np.outer(a2, delta3)\n",
        "            b2 = b2 - alpha * delta3\n",
        "            \n",
        "            # calculo de gradiente para W1 por retropropagación\n",
        "            delta2 = np.dot(W2, delta3) * derivada_sigmoide(z2)\n",
        "            W1 = W1 - alpha * np.outer(X[j], delta2)\n",
        "            b1 = b1 - alpha * delta2\n",
        "            \n",
        "            y_predicha[j] = y_hat\n",
        "            \n",
        "        # calcula la pérdida en época\n",
        "        perdidas[i] = entropia_cruzada_binaria(y, y_predicha)\n",
        "        exactitudes[i] = exactitud(y, np.round(y_predicha))\n",
        "        print('Epoch {0}: Pérdida = {1} Exactitud = {2}'.format(i, \n",
        "                                                              perdidas[i], \n",
        "                                                              exactitudes[i]))\n",
        "        print('W1 = {0}'.format(W1))\n",
        "        print('W2 = {0}'.format(W2))\n",
        "            \n",
        "    return W1, W2, perdidas, exactitudes"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr1HownICHf9",
        "outputId": "557c44b1-31ad-47e8-e487-10dcbd42672b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "W1, W2, perdidas, exactitudes = retropropagacion_zeros(X, \n",
        "                                                       y, \n",
        "                                                       alpha = 1.0,\n",
        "                                                       n_epocas = 5,\n",
        "                                                       n_ocultas = 10)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: Pérdida = 2.383938269247035 Exactitud = 100.0\n",
            "W1 = [[ 0.0015006   0.0015006   0.0015006   0.0015006   0.0015006   0.0015006\n",
            "   0.0015006   0.0015006   0.0015006   0.0015006 ]\n",
            " [-0.00013937 -0.00013937 -0.00013937 -0.00013937 -0.00013937 -0.00013937\n",
            "  -0.00013937 -0.00013937 -0.00013937 -0.00013937]]\n",
            "W2 = [[0.00877009]\n",
            " [0.00877009]\n",
            " [0.00877009]\n",
            " [0.00877009]\n",
            " [0.00877009]\n",
            " [0.00877009]\n",
            " [0.00877009]\n",
            " [0.00877009]\n",
            " [0.00877009]\n",
            " [0.00877009]]\n",
            "Epoch 1: Pérdida = 2.3940150790588524 Exactitud = 75.0\n",
            "W1 = [[3.07843345e-03 3.07843345e-03 3.07843345e-03 3.07843345e-03\n",
            "  3.07843345e-03 3.07843345e-03 3.07843345e-03 3.07843345e-03\n",
            "  3.07843345e-03 3.07843345e-03]\n",
            " [6.32326402e-05 6.32326402e-05 6.32326402e-05 6.32326402e-05\n",
            "  6.32326402e-05 6.32326402e-05 6.32326402e-05 6.32326402e-05\n",
            "  6.32326402e-05 6.32326402e-05]]\n",
            "W2 = [[0.03001629]\n",
            " [0.03001629]\n",
            " [0.03001629]\n",
            " [0.03001629]\n",
            " [0.03001629]\n",
            " [0.03001629]\n",
            " [0.03001629]\n",
            " [0.03001629]\n",
            " [0.03001629]\n",
            " [0.03001629]]\n",
            "Epoch 2: Pérdida = 2.454375736775318 Exactitud = 50.0\n",
            "W1 = [[0.00535668 0.00535668 0.00535668 0.00535668 0.00535668 0.00535668\n",
            "  0.00535668 0.00535668 0.00535668 0.00535668]\n",
            " [0.00153339 0.00153339 0.00153339 0.00153339 0.00153339 0.00153339\n",
            "  0.00153339 0.00153339 0.00153339 0.00153339]]\n",
            "W2 = [[0.07968444]\n",
            " [0.07968444]\n",
            " [0.07968444]\n",
            " [0.07968444]\n",
            " [0.07968444]\n",
            " [0.07968444]\n",
            " [0.07968444]\n",
            " [0.07968444]\n",
            " [0.07968444]\n",
            " [0.07968444]]\n",
            "Epoch 3: Pérdida = 2.7471355126181987 Exactitud = 50.0\n",
            "W1 = [[0.01036404 0.01036404 0.01036404 0.01036404 0.01036404 0.01036404\n",
            "  0.01036404 0.01036404 0.01036404 0.01036404]\n",
            " [0.00645381 0.00645381 0.00645381 0.00645381 0.00645381 0.00645381\n",
            "  0.00645381 0.00645381 0.00645381 0.00645381]]\n",
            "W2 = [[0.17472438]\n",
            " [0.17472438]\n",
            " [0.17472438]\n",
            " [0.17472438]\n",
            " [0.17472438]\n",
            " [0.17472438]\n",
            " [0.17472438]\n",
            " [0.17472438]\n",
            " [0.17472438]\n",
            " [0.17472438]]\n",
            "Epoch 4: Pérdida = 3.5902678548185696 Exactitud = 50.0\n",
            "W1 = [[0.01745773 0.01745773 0.01745773 0.01745773 0.01745773 0.01745773\n",
            "  0.01745773 0.01745773 0.01745773 0.01745773]\n",
            " [0.01361346 0.01361346 0.01361346 0.01361346 0.01361346 0.01361346\n",
            "  0.01361346 0.01361346 0.01361346 0.01361346]]\n",
            "W2 = [[0.28213958]\n",
            " [0.28213958]\n",
            " [0.28213958]\n",
            " [0.28213958]\n",
            " [0.28213958]\n",
            " [0.28213958]\n",
            " [0.28213958]\n",
            " [0.28213958]\n",
            " [0.28213958]\n",
            " [0.28213958]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}